{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2017-12-10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 2: Resilient Distributed Datasets - Transformations\n",
    "\n",
    "In this section we will look at the Resilient Distributed Datasets (RDDs) and the transformations available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "### Parallelize a collection\n",
    "#### 1000 random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_numbers = sc.parallelize([np.random.rand() for _ in range(1000)], 4)\n",
    "random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mark', 43, '6\\'1\"'), ('Stella', 23, '5\\'6\"')]"
     ]
    }
   ],
   "source": [
    "rdd_random_tuples = sc.parallelize([\n",
    "      (\"Mark\",  43, \"6'1\\\"\")\n",
    "    , (\"Stella\",23, \"5'6\\\"\")\n",
    "    , (\"Skye\",   6, \"3'11\\\"\")\n",
    "    , (\"Albert\", 1, \"2'7\\\"\")\n",
    "])\n",
    "rdd_random_tuples.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from files\n",
    "The `sample_data.csv` was created based on the data found here: http://www.contextures.com/xlSampleData01.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate,Region,Rep,Item,Units,UnitCost,Total', '1/6/16,East,Jones,Pencil,95,1.99,189.05']"
     ]
    }
   ],
   "source": [
    "rdd_from_file = sc.textFile('../data/sample_data.csv', 4)\n",
    "rdd_from_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema of an RDD\n",
    "### Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_valid_structured = sc.parallelize([\n",
    "      ('Tom', 0)\n",
    "    , ('Spark', 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured data\n",
    "Sample Linux log data, source: https://www.cyberciti.biz/faq/linux-log-files-location-and-how-do-i-view-logs-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_valid_unstructured = sc.parallelize([\n",
    "      ('Jul 17 22:04:25 router  dnsprobe[276]: dns query failed')\n",
    "    , ('Jul 17 22:04:29 router last message repeated 2 times')  \n",
    "    , ('Jul 17 22:04:29 router  dnsprobe[276]: Primary DNS server Is Down... Switching To Secondary DNS server')\n",
    "    , ('Jul 17 22:05:08 router  dnsprobe[276]: Switching Back To Primary DNS server')\n",
    "    , ('Jul 17 22:26:11 debian -- MARK --')\n",
    "    , ('Jul 17 22:46:11 debian -- MARK --')\n",
    "    , ('Jul 17 22:47:36 router  -- MARK --')\n",
    "    , ('Jul 17 22:47:36 router  dnsprobe[276]: dns query failed')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_valid_heterogeneous = sc.parallelize([\n",
    "      ('Toll invoice number', 'TRE2321')\n",
    "    , {'Last bill balance': 0.00, 'New bill balance': 23.92}\n",
    "    , ['2017-11-12 12:32:34PM', '2017-11-14 1:32:56PM']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding lazy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_only = rdd_valid_structured.map(lambda element: element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tom', 'Spark']"
     ]
    }
   ],
   "source": [
    "names_only.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .map(...) transformation\n",
    "### Using lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate', 'Region', 'Rep', 'Item', 'Units', 'UnitCost', 'Total']\n",
      "['1/6/16', 'East', 'Jones', 'Pencil', '95', '1.99', '189.05']"
     ]
    }
   ],
   "source": [
    "def splitOnComma(inputString):\n",
    "    return inputString.split(',')\n",
    "\n",
    "for el in rdd_from_file.map(splitOnComma).take(2):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate', 'Region', 'Rep', 'Item', 'Units', 'UnitCost', 'Total']\n",
      "['1/6/16', 'East', 'Jones', 'Pencil', '95', '1.99', '189.05']"
     ]
    }
   ],
   "source": [
    "for el in rdd_from_file.map(lambda element: element.split(',')).take(2):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OrderDate', 'Region', 'Rep', 'Item', -666, -666, -666], ['1/6/16', 'East', 'Jones', 'Pencil', 95.0, 1.99, 189.05]]"
     ]
    }
   ],
   "source": [
    "def convertToFloat(inputString):\n",
    "    try:\n",
    "        return float(inputString)\n",
    "    except:\n",
    "        return -666\n",
    "\n",
    "to_filter = rdd_from_file \\\n",
    "    .map(lambda element: element.split(',')) \\\n",
    "    .map(lambda element:\n",
    "         [e for e in element[:4]] +\n",
    "         [convertToFloat(e) for e in element[4:]]\n",
    "    )\n",
    "    \n",
    "to_filter.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .filter(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1/6/16', 'East', 'Jones', 'Pencil', 95.0, 1.99, 189.05], ['2017-03-02', 'Central', 'Kivell', 'Binder', 50.0, 19.99, 999.5]]"
     ]
    }
   ],
   "source": [
    "filtered = to_filter.filter(lambda element: element[4] != -666)\n",
    "filtered.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .flatMap(...) transformation\n",
    "### .map to flatMap comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 30], [2, 40], [3, 50], [4, 60]]\n",
      "[1, 30, 2, 40, 3, 50, 4, 60]"
     ]
    }
   ],
   "source": [
    "def mapExample(inputList):\n",
    "    outputList = []\n",
    "    \n",
    "    for item in inputList:\n",
    "        temp = item.copy()\n",
    "        temp[1] *= 10\n",
    "        \n",
    "        outputList.append(temp)\n",
    "    return outputList\n",
    "\n",
    "def flatMapExample(inputList):\n",
    "    outputList = []\n",
    "    \n",
    "    for item in inputList:\n",
    "        temp = item.copy()\n",
    "        temp[1] *= 10\n",
    "        \n",
    "        outputList += temp\n",
    "        \n",
    "    return outputList\n",
    "\n",
    "sampleList = [\n",
    "      [1, 3]\n",
    "    , [2, 4]\n",
    "    , [3, 5]\n",
    "    , [4, 6]\n",
    "]\n",
    "\n",
    "print(mapExample(sampleList))\n",
    "print(flatMapExample(sampleList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05]], []]"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def parseCSVRow(inputRow):\n",
    "    try:\n",
    "        rowSplit = inputRow.split(',')\n",
    "        rowSplit[0] = dt.datetime.strptime(rowSplit[0], '%m/%d/%y')\n",
    "        rowSplit[4] = int(rowSplit[4])\n",
    "        \n",
    "        for i in [5,6]:\n",
    "            rowSplit[i] = float(rowSplit[i])\n",
    "        \n",
    "        return [rowSplit]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "rdd_from_file.map(parseCSVRow).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05]], [[datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64]], [[datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73]]]"
     ]
    }
   ],
   "source": [
    "rdd_from_file.map(parseCSVRow).filter(lambda element: len(element) > 0).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05], [datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64], [datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73]]"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean = rdd_from_file.flatMap(parseCSVRow)\n",
    "rdd_from_file_clean.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .distinct(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = rdd_from_file_clean.map(lambda element: element[3]).distinct()\n",
    "items.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_clean.map(lambda element: element[2]).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .sample(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_clean.sample(False, 0.2).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .join(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = sc.parallelize([\n",
    "      ('East', 'Boston')\n",
    "    , ('Central', 'Chicago')\n",
    "    , ('West', 'Seattle')\n",
    "])\n",
    "\n",
    "rdd_from_file_clean \\\n",
    "    .map(lambda element: (element[1], element)) \\\n",
    "    .join(cities) \\\n",
    "    .map(lambda element: element[1][0] + [element[1][1]]) \\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .repartition(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_clean.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_repartitioned = rdd_from_file.repartition(2)\n",
    "rdd_from_file_repartitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redd_from_file_repartitioned_sorted = rdd_from_file_clean \\\n",
    "    .map(lambda element: (element[4], element)) \\\n",
    "    .repartitionAndSortWithinPartitions(2, lambda x: x) \\\n",
    "    .map(lambda element: tuple(element[1]))\n",
    "\n",
    "redd_from_file_repartitioned_sorted.glom() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redd_from_file_repartitioned_sorted.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
